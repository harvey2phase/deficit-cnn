{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["fbHvrLUNg4U0","FXvm1LOb9NI6","RNyeOj71lHIC","w7WvBkAj_BEi"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7fcY2b7xlsIQ","colab_type":"text"},"source":["## Mount Drives, Import Packages, Setup"]},{"cell_type":"code","metadata":{"id":"GmYpfOzcpK2j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"273a460e-d87e-4a10-b9ef-c550c5b1b3c7","executionInfo":{"status":"ok","timestamp":1557510816458,"user_tz":180,"elapsed":36464,"user":{"displayName":"Harvey Wang","photoUrl":"https://lh5.googleusercontent.com/-v6ZFtxMCliI/AAAAAAAAAAI/AAAAAAAABzQ/ghEL5GjlxHQ/s64/photo.jpg","userId":"14547178415595556415"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","research_path = '/content/gdrive/Computers/My MacBook Pro/Summer Research/'\n","data_path = research_path + 'processed_data/matrix_50nodes/'"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R-BM3_VHIjup","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","import keras"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tr7I0fuY46ds","colab_type":"code","colab":{}},"source":["#tf.logging.set_verbosity(tf.logging.INFO)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fbHvrLUNg4U0","colab_type":"text"},"source":["##Model function for CNN"]},{"cell_type":"markdown","metadata":{"id":"FXvm1LOb9NI6","colab_type":"text"},"source":["###Mnist CNN"]},{"cell_type":"code","metadata":{"id":"0BlpWhnZEj1u","colab_type":"code","colab":{}},"source":["def cnn_model_fn(features, labels, mode):\n","\n","    # Input Layer\n","    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n","    # MNIST images are 28x28 pixels, and have one color channel\n","    input_layer = tf.reshape(features, [-1, 28, 28, 1])\n","    #input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n","    \n","    print(input_layer)\n","    \n","    ''' Convolutional Layer #1'''\n","    # Computes 32 features using a 5x5 filter with ReLU activation.\n","    # Padding is added to preserve width and height.\n","    # Input Tensor Shape: [batch_size, 28, 28, 1]\n","    # Output Tensor Shape: [batch_size, 28, 28, 32]\n","    conv1 = tf.layers.conv2d(\n","        inputs = input_layer,\n","        filters = 32,\n","        kernel_size = [5, 5],\n","        padding = \"same\",\n","        activation = tf.nn.relu\n","    )\n","\n","    ''' Pooling Layer #1 '''\n","    # First max pooling layer with a 2x2 filter and stride of 2\n","    # Input Tensor Shape: [batch_size, 28, 28, 32]\n","    # Output Tensor Shape: [batch_size, 14, 14, 32]\n","    pool1 = tf.layers.max_pooling2d(\n","        inputs = conv1,\n","        pool_size = [2, 2],\n","        strides = 2\n","    )\n","    \n","    ''' Convolutional Layer #2 '''\n","    # Computes 64 features using a 5x5 filter.\n","    # Padding is added to preserve width and height.\n","    # Input Tensor Shape: [batch_size, 14, 14, 32]\n","    # Output Tensor Shape: [batch_size, 14, 14, 64]\n","    conv2 = tf.layers.conv2d(\n","        inputs = pool1,\n","        filters = 64,\n","        kernel_size = [5, 5],\n","        padding = \"same\",\n","        activation = tf.nn.relu\n","    )\n","\n","    ''' Pooling Layer #2 '''\n","    # Second max pooling layer with a 2x2 filter and stride of 2\n","    # Input Tensor Shape: [batch_size, 14, 14, 64]\n","    # Output Tensor Shape: [batch_size, 7, 7, 64]\n","    pool2 = tf.layers.max_pooling2d(\n","        inputs = conv2,\n","        pool_size = [2, 2],\n","        strides = 2\n","    )\n","\n","    ''' Flatten tensor into a batch of vectors '''\n","    # Input Tensor Shape: [batch_size, 7, 7, 64]\n","    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n","    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n","\n","    ''' Dense Layer '''\n","    # Densely connected layer with 1024 neurons\n","    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n","    # Output Tensor Shape: [batch_size, 1024]\n","    dense = tf.layers.dense(\n","        inputs = pool2_flat,\n","        units = 1024,\n","        activation = tf.nn.relu\n","    )\n","\n","    ''' Add dropout operation '''\n","    # 0.6 probability that element will be kept\n","    dropout = tf.layers.dropout(\n","        inputs = dense,\n","        rate = 0.4,\n","        training = mode == tf.estimator.ModeKeys.TRAIN\n","    )\n","\n","\n","    ''' Logits layer '''\n","    # Input Tensor Shape: [batch_size, 1024]\n","    # Output Tensor Shape: [batch_size, 10]\n","    logits = tf.layers.dense(\n","        inputs = dropout,\n","        units = 10\n","    )\n","\n","    predictions = {\n","        # Generate predictions (for PREDICT and EVAL mode)\n","        \"classes\": tf.argmax(input=logits, axis=1),\n","        \n","        # Add `softmax_tensor` to the graph.\n","        # It is used for PREDICT and by the `logging_hook`.\n","        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n","    }\n","\n","    if mode == tf.estimator.ModeKeys.PREDICT:\n","        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n","\n","    # Calculate Loss (for both TRAIN and EVAL modes)\n","    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n","\n","    # Configure the Training Op (for TRAIN mode)\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        \n","        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001)\n","        train_op = optimizer.minimize(\n","            loss = loss,\n","            global_step = tf.train.get_global_step()\n","        )\n","        \n","        return tf.estimator.EstimatorSpec(\n","            mode = mode,\n","            loss = loss,\n","            train_op = train_op\n","        )\n","\n","    # Add evaluation metrics (for EVAL mode)\n","    eval_metric_ops = {\n","        \"accuracy\": tf.metrics.accuracy(\n","            labels = labels,\n","            predictions = predictions[\"classes\"]\n","        )\n","    }\n","    \n","    return tf.estimator.EstimatorSpec(\n","        mode = mode,\n","        loss = loss,\n","        eval_metric_ops = eval_metric_ops\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H_iLx32nUkaX","colab_type":"text"},"source":["### Deficits CNN"]},{"cell_type":"code","metadata":{"id":"PDfkCpG5g5Cm","colab_type":"code","colab":{}},"source":["def deficit_cnn_model(features, labels, mode):\n","\n","    # Input Layer\n","    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n","    # MNIST images are 28x28 pixels, and have one color channel\n","    input_layer = tf.reshape(features, [-1, 50, 120, 1])\n","\n","    ''' Convolutional Layer #1'''\n","    # Computes 32 features using a 5x5 filter with ReLU activation.\n","    # Padding is added to preserve width and height.\n","    # Input Tensor Shape: [batch_size, 50, 120, 1]\n","    # Output Tensor Shape: [batch_size, 50, 120, 32]\n","    conv1 = keras.layers.conv2d(\n","        inputs = input_layer,\n","        filters = 32,\n","        kernel_size = [5, 5],\n","        padding = \"same\",\n","        activation = tf.nn.relu\n","    )\n","\n","    ''' Pooling Layer #1 '''\n","    # First max pooling layer with a 2x2 filter and stride of 2\n","    # Input Tensor Shape: [batch_size, 28, 28, 32]\n","    # Output Tensor Shape: [batch_size, 14, 14, 32]\n","    pool1 = tf.layers.max_pooling2d(\n","        inputs = conv1,\n","        pool_size = [2, 2],\n","        strides = 2\n","    )\n","    \n","    ''' Convolutional Layer #2 '''\n","    # Computes 64 features using a 5x5 filter.\n","    # Padding is added to preserve width and height.\n","    # Input Tensor Shape: [batch_size, 14, 14, 32]\n","    # Output Tensor Shape: [batch_size, 14, 14, 64]\n","    conv2 = tf.layers.conv2d(\n","        inputs = pool1,\n","        filters = 64,\n","        kernel_size = [5, 5],\n","        padding = \"same\",\n","        activation = tf.nn.relu\n","    )\n","\n","    ''' Pooling Layer #2 '''\n","    # Second max pooling layer with a 2x2 filter and stride of 2\n","    # Input Tensor Shape: [batch_size, 14, 14, 64]\n","    # Output Tensor Shape: [batch_size, 7, 7, 64]\n","    pool2 = tf.layers.max_pooling2d(\n","        inputs = conv2,\n","        pool_size = [2, 2],\n","        strides = 2\n","    )\n","\n","    ''' Flatten tensor into a batch of vectors '''\n","    # Input Tensor Shape: [batch_size, 7, 7, 64]\n","    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n","    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n","\n","    ''' Dense Layer '''\n","    # Densely connected layer with 1024 neurons\n","    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n","    # Output Tensor Shape: [batch_size, 1024]\n","    dense = tf.layers.dense(\n","        inputs = pool2_flat,\n","        units = 1024,\n","        activation = tf.nn.relu\n","    )\n","\n","    ''' Add dropout operation '''\n","    # 0.6 probability that element will be kept\n","    dropout = tf.layers.dropout(\n","        inputs = dense,\n","        rate = 0.4,\n","        training = mode == tf.estimator.ModeKeys.TRAIN\n","    )\n","    \n","\n","    ''' Logits layer '''\n","    # Input Tensor Shape: [batch_size, 1024]\n","    # Output Tensor Shape: [batch_size, 10]\n","    logits = tf.layers.dense(\n","        inputs = dropout,\n","        units = 10\n","    )\n","\n","    predictions = {\n","        # Generate predictions (for PREDICT and EVAL mode)\n","        \"classes\": tf.argmax(input=logits, axis=1),\n","        \n","        # Add `softmax_tensor` to the graph.\n","        # It is used for PREDICT and by the `logging_hook`.\n","        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n","    }\n","\n","    if mode == tf.estimator.ModeKeys.PREDICT:\n","        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n","\n","    # Calculate Loss (for both TRAIN and EVAL modes)\n","    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n","\n","    # Configure the Training Op (for TRAIN mode)\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        \n","        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001)\n","        train_op = optimizer.minimize(\n","            loss = loss,\n","            global_step = tf.train.get_global_step()\n","        )\n","        \n","        return tf.estimator.EstimatorSpec(\n","            mode = mode,\n","            loss = loss,\n","            train_op = train_op\n","        )\n","\n","    # Add evaluation metrics (for EVAL mode)\n","    eval_metric_ops = {\n","        \"accuracy\": tf.metrics.accuracy(\n","            labels = labels,\n","            predictions = predictions[\"classes\"]\n","        )\n","    }\n","    \n","    return tf.estimator.EstimatorSpec(\n","        mode = mode,\n","        loss = loss,\n","        eval_metric_ops = eval_metric_ops\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HSyPV9W0k6JZ","colab_type":"text"},"source":["## Load Data\n","Load training and evaluation data"]},{"cell_type":"markdown","metadata":{"id":"ha6B6AxD-FjX","colab_type":"text"},"source":["### Mnist Data"]},{"cell_type":"code","metadata":{"id":"u4HnQSblVGPP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"c824074f-2c2a-4aaa-e740-97c97481ae4b","executionInfo":{"status":"ok","timestamp":1557507240397,"user_tz":180,"elapsed":704,"user":{"displayName":"Harvey Wang","photoUrl":"https://lh5.googleusercontent.com/-v6ZFtxMCliI/AAAAAAAAAAI/AAAAAAAABzQ/ghEL5GjlxHQ/s64/photo.jpg","userId":"14547178415595556415"}}},"source":["mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n","\n","train_data = mnist.train.images  # Returns np.array\n","train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n","\n","eval_data = mnist.test.images  # Returns np.array\n","eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n","\n","f = open(path + 'test.txt')\n","print(train_data.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Extracting MNIST-data/train-images-idx3-ubyte.gz\n","Extracting MNIST-data/train-labels-idx1-ubyte.gz\n","Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n","Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n","(55000, 784)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JTvGEY1W-MnJ","colab_type":"text"},"source":["### Deficit Data"]},{"cell_type":"markdown","metadata":{"id":"grRq6ptz_j5I","colab_type":"text"},"source":["####Load individual data at death age $80 < d < 90$"]},{"cell_type":"code","metadata":{"id":"N8Ed_A9yl-NC","colab_type":"code","colab":{}},"source":["# filename = '69.txt'\n","data = \n","\n","# ind = np.loadtxt(path+filename)\n","\n","# time = ind[:, 0]\n","# node = ind[:, 1]\n","# state = ind[:, 2]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zI-hagvS-2YD","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"RNyeOj71lHIC","colab_type":"text"},"source":["## Create the Estimator\n"]},{"cell_type":"code","metadata":{"id":"iEfLJfhbkvTS","colab_type":"code","colab":{}},"source":["mnist_classifier = tf.estimator.Estimator(\n","    model_fn = cnn_model_fn,\n","    #model_dir=\"/tmp/mnist_convnet_model\"\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w7WvBkAj_BEi","colab_type":"text"},"source":["##Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"G8HOs5uolO8N","colab_type":"text"},"source":["### Set up logging for predictions.\n","\n","Log the values in the \"Softmax\" tensor with label \"probabilities\""]},{"cell_type":"code","metadata":{"id":"YV69DYwnlMXE","colab_type":"code","colab":{}},"source":["tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n","logging_hook = tf.train.LoggingTensorHook(\n","    tensors = tensors_to_log,\n","    every_n_iter = 50\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J8ZGkIhClaox","colab_type":"text"},"source":["###Train the model"]},{"cell_type":"code","metadata":{"id":"gtHHvfpQSODs","colab_type":"code","colab":{}},"source":["print(train_data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WTk96pYqZ4E","colab_type":"code","colab":{}},"source":["input_layer = tf.reshape(train_data, [-1, 28, 28, 1])\n","print(input_layer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pnc1CR2AlbSe","colab_type":"code","colab":{}},"source":["train_input_fn = tf.estimator.inputs.numpy_input_fn(\n","    x = train_data,\n","    y = train_labels,\n","    batch_size = 100,\n","    num_epochs = None,\n","    shuffle = True\n",")\n","mnist_classifier.train(\n","    input_fn = train_input_fn,\n","    steps = 20000,\n","    hooks = [logging_hook]\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-mRbXr40mX6_","colab_type":"text"},"source":["###Evalute the model and print results"]},{"cell_type":"code","metadata":{"id":"hlWqYSqmme41","colab_type":"code","colab":{}},"source":["eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n","    x = {\"x\": eval_data},\n","    y = eval_labels,\n","    num_epochs = 1,\n","    shuffle = False\n",")\n","eval_results = mnist_classifier.evaluate(input_fn = eval_input_fn)\n","print(eval_results)"],"execution_count":0,"outputs":[]}]}