{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["H_iLx32nUkaX"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"7fcY2b7xlsIQ","colab_type":"text"},"cell_type":"markdown","source":["### Mount Drives, Import Packages, Setup"]},{"metadata":{"id":"GmYpfOzcpK2j","colab_type":"code","colab":{}},"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","\n","# path = '/content/gdrive/My Drive/Honours/Processed Data/individual_data/'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"R-BM3_VHIjup","colab_type":"code","colab":{}},"cell_type":"code","source":["from __future__ import absolute_import, division, print_function\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","\n","tf.logging.set_verbosity(tf.logging.INFO)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fbHvrLUNg4U0","colab_type":"text"},"cell_type":"markdown","source":["###Model function for CNN"]},{"metadata":{"id":"FXvm1LOb9NI6","colab_type":"text"},"cell_type":"markdown","source":["####Mnist CNN"]},{"metadata":{"id":"0BlpWhnZEj1u","colab_type":"code","colab":{}},"cell_type":"code","source":["def cnn_model_fn(features, labels, mode):\n","\n","    # Input Layer\n","    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n","    # MNIST images are 28x28 pixels, and have one color channel\n","    input_layer = tf.reshape(features, [-1, 28, 28, 1])\n","    #input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n","    \n","    print(input_layer)\n","    \n","    ''' Convolutional Layer #1'''\n","    # Computes 32 features using a 5x5 filter with ReLU activation.\n","    # Padding is added to preserve width and height.\n","    # Input Tensor Shape: [batch_size, 28, 28, 1]\n","    # Output Tensor Shape: [batch_size, 28, 28, 32]\n","    conv1 = tf.layers.conv2d(\n","        inputs = input_layer,\n","        filters = 32,\n","        kernel_size = [5, 5],\n","        padding = \"same\",\n","        activation = tf.nn.relu\n","    )\n","\n","    ''' Pooling Layer #1 '''\n","    # First max pooling layer with a 2x2 filter and stride of 2\n","    # Input Tensor Shape: [batch_size, 28, 28, 32]\n","    # Output Tensor Shape: [batch_size, 14, 14, 32]\n","    pool1 = tf.layers.max_pooling2d(\n","        inputs = conv1,\n","        pool_size = [2, 2],\n","        strides = 2\n","    )\n","    \n","    ''' Convolutional Layer #2 '''\n","    # Computes 64 features using a 5x5 filter.\n","    # Padding is added to preserve width and height.\n","    # Input Tensor Shape: [batch_size, 14, 14, 32]\n","    # Output Tensor Shape: [batch_size, 14, 14, 64]\n","    conv2 = tf.layers.conv2d(\n","        inputs = pool1,\n","        filters = 64,\n","        kernel_size = [5, 5],\n","        padding = \"same\",\n","        activation = tf.nn.relu\n","    )\n","\n","    ''' Pooling Layer #2 '''\n","    # Second max pooling layer with a 2x2 filter and stride of 2\n","    # Input Tensor Shape: [batch_size, 14, 14, 64]\n","    # Output Tensor Shape: [batch_size, 7, 7, 64]\n","    pool2 = tf.layers.max_pooling2d(\n","        inputs = conv2,\n","        pool_size = [2, 2],\n","        strides = 2\n","    )\n","\n","    ''' Flatten tensor into a batch of vectors '''\n","    # Input Tensor Shape: [batch_size, 7, 7, 64]\n","    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n","    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n","\n","    ''' Dense Layer '''\n","    # Densely connected layer with 1024 neurons\n","    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n","    # Output Tensor Shape: [batch_size, 1024]\n","    dense = tf.layers.dense(\n","        inputs = pool2_flat,\n","        units = 1024,\n","        activation = tf.nn.relu\n","    )\n","\n","    ''' Add dropout operation '''\n","    # 0.6 probability that element will be kept\n","    dropout = tf.layers.dropout(\n","        inputs = dense,\n","        rate = 0.4,\n","        training = mode == tf.estimator.ModeKeys.TRAIN\n","    )\n","\n","\n","    ''' Logits layer '''\n","    # Input Tensor Shape: [batch_size, 1024]\n","    # Output Tensor Shape: [batch_size, 10]\n","    logits = tf.layers.dense(\n","        inputs = dropout,\n","        units = 10\n","    )\n","\n","    predictions = {\n","        # Generate predictions (for PREDICT and EVAL mode)\n","        \"classes\": tf.argmax(input=logits, axis=1),\n","        \n","        # Add `softmax_tensor` to the graph.\n","        # It is used for PREDICT and by the `logging_hook`.\n","        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n","    }\n","\n","    if mode == tf.estimator.ModeKeys.PREDICT:\n","        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n","\n","    # Calculate Loss (for both TRAIN and EVAL modes)\n","    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n","\n","    # Configure the Training Op (for TRAIN mode)\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        \n","        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001)\n","        train_op = optimizer.minimize(\n","            loss = loss,\n","            global_step = tf.train.get_global_step()\n","        )\n","        \n","        return tf.estimator.EstimatorSpec(\n","            mode = mode,\n","            loss = loss,\n","            train_op = train_op\n","        )\n","\n","    # Add evaluation metrics (for EVAL mode)\n","    eval_metric_ops = {\n","        \"accuracy\": tf.metrics.accuracy(\n","            labels = labels,\n","            predictions = predictions[\"classes\"]\n","        )\n","    }\n","    \n","    return tf.estimator.EstimatorSpec(\n","        mode = mode,\n","        loss = loss,\n","        eval_metric_ops = eval_metric_ops\n","    )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H_iLx32nUkaX","colab_type":"text"},"cell_type":"markdown","source":["#### Deficits CNN"]},{"metadata":{"id":"PDfkCpG5g5Cm","colab_type":"code","colab":{}},"cell_type":"code","source":["def deficit_cnn_model(features, labels, mode):\n","\n","    # Input Layer\n","    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n","    # MNIST images are 28x28 pixels, and have one color channel\n","    input_layer = tf.reshape(features, [-1, 28, 28, 1])\n","\n","    ''' Convolutional Layer #1'''\n","    # Computes 32 features using a 5x5 filter with ReLU activation.\n","    # Padding is added to preserve width and height.\n","    # Input Tensor Shape: [batch_size, 28, 28, 1]\n","    # Output Tensor Shape: [batch_size, 28, 28, 32]\n","    conv1 = tf.layers.conv2d(\n","        inputs = input_layer,\n","        filters = 32,\n","        kernel_size = [5, 5],\n","        padding = \"same\",\n","        activation = tf.nn.relu\n","    )\n","\n","    ''' Pooling Layer #1 '''\n","    # First max pooling layer with a 2x2 filter and stride of 2\n","    # Input Tensor Shape: [batch_size, 28, 28, 32]\n","    # Output Tensor Shape: [batch_size, 14, 14, 32]\n","    pool1 = tf.layers.max_pooling2d(\n","        inputs = conv1,\n","        pool_size = [2, 2],\n","        strides = 2\n","    )\n","    \n","    ''' Convolutional Layer #2 '''\n","    # Computes 64 features using a 5x5 filter.\n","    # Padding is added to preserve width and height.\n","    # Input Tensor Shape: [batch_size, 14, 14, 32]\n","    # Output Tensor Shape: [batch_size, 14, 14, 64]\n","    conv2 = tf.layers.conv2d(\n","        inputs = pool1,\n","        filters = 64,\n","        kernel_size = [5, 5],\n","        padding = \"same\",\n","        activation = tf.nn.relu\n","    )\n","\n","    ''' Pooling Layer #2 '''\n","    # Second max pooling layer with a 2x2 filter and stride of 2\n","    # Input Tensor Shape: [batch_size, 14, 14, 64]\n","    # Output Tensor Shape: [batch_size, 7, 7, 64]\n","    pool2 = tf.layers.max_pooling2d(\n","        inputs = conv2,\n","        pool_size = [2, 2],\n","        strides = 2\n","    )\n","\n","    ''' Flatten tensor into a batch of vectors '''\n","    # Input Tensor Shape: [batch_size, 7, 7, 64]\n","    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n","    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n","\n","    ''' Dense Layer '''\n","    # Densely connected layer with 1024 neurons\n","    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n","    # Output Tensor Shape: [batch_size, 1024]\n","    dense = tf.layers.dense(\n","        inputs = pool2_flat,\n","        units = 1024,\n","        activation = tf.nn.relu\n","    )\n","\n","    ''' Add dropout operation '''\n","    # 0.6 probability that element will be kept\n","    dropout = tf.layers.dropout(\n","        inputs = dense,\n","        rate = 0.4,\n","        training = mode == tf.estimator.ModeKeys.TRAIN\n","    )\n","\n","\n","    ''' Logits layer '''\n","    # Input Tensor Shape: [batch_size, 1024]\n","    # Output Tensor Shape: [batch_size, 10]\n","    logits = tf.layers.dense(\n","        inputs = dropout,\n","        units = 10\n","    )\n","\n","    predictions = {\n","        # Generate predictions (for PREDICT and EVAL mode)\n","        \"classes\": tf.argmax(input=logits, axis=1),\n","        \n","        # Add `softmax_tensor` to the graph.\n","        # It is used for PREDICT and by the `logging_hook`.\n","        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n","    }\n","\n","    if mode == tf.estimator.ModeKeys.PREDICT:\n","        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n","\n","    # Calculate Loss (for both TRAIN and EVAL modes)\n","    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n","\n","    # Configure the Training Op (for TRAIN mode)\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        \n","        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001)\n","        train_op = optimizer.minimize(\n","            loss = loss,\n","            global_step = tf.train.get_global_step()\n","        )\n","        \n","        return tf.estimator.EstimatorSpec(\n","            mode = mode,\n","            loss = loss,\n","            train_op = train_op\n","        )\n","\n","    # Add evaluation metrics (for EVAL mode)\n","    eval_metric_ops = {\n","        \"accuracy\": tf.metrics.accuracy(\n","            labels = labels,\n","            predictions = predictions[\"classes\"]\n","        )\n","    }\n","    \n","    return tf.estimator.EstimatorSpec(\n","        mode = mode,\n","        loss = loss,\n","        eval_metric_ops = eval_metric_ops\n","    )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HSyPV9W0k6JZ","colab_type":"text"},"cell_type":"markdown","source":["### Load Data\n","Load training and evaluation data"]},{"metadata":{"id":"u4HnQSblVGPP","colab_type":"code","outputId":"485131a2-dbb0-44f7-cf03-c6ccf78255ad","executionInfo":{"status":"ok","timestamp":1549672872046,"user_tz":240,"elapsed":1777,"user":{"displayName":"Harvey Wang","photoUrl":"https://lh5.googleusercontent.com/-v6ZFtxMCliI/AAAAAAAAAAI/AAAAAAAABzQ/ghEL5GjlxHQ/s64/photo.jpg","userId":"14547178415595556415"}},"colab":{"base_uri":"https://localhost:8080/","height":581}},"cell_type":"code","source":["mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n","\n","train_data = mnist.train.images  # Returns np.array\n","train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n","\n","eval_data = mnist.test.images  # Returns np.array\n","eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-4-b2fa85a8599e>:1: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please write your own downloading logic.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting MNIST-data/train-images-idx3-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting MNIST-data/train-labels-idx1-ubyte.gz\n","Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n","Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"],"name":"stdout"}]},{"metadata":{"id":"N8Ed_A9yl-NC","colab_type":"code","colab":{}},"cell_type":"code","source":["# filename = '69.txt'\n","\n","# ind = np.loadtxt(path+filename)\n","\n","# time = ind[:, 0]\n","# node = ind[:, 1]\n","# state = ind[:, 2]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RNyeOj71lHIC","colab_type":"text"},"cell_type":"markdown","source":["### Create the Estimator\n"]},{"metadata":{"id":"iEfLJfhbkvTS","colab_type":"code","outputId":"1e001579-4d9d-4cc9-bbf0-b623dcb48345","executionInfo":{"status":"ok","timestamp":1549672142850,"user_tz":240,"elapsed":734,"user":{"displayName":"Harvey Wang","photoUrl":"https://lh5.googleusercontent.com/-v6ZFtxMCliI/AAAAAAAAAAI/AAAAAAAABzQ/ghEL5GjlxHQ/s64/photo.jpg","userId":"14547178415595556415"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"cell_type":"code","source":["mnist_classifier = tf.estimator.Estimator(\n","    model_fn = cnn_model_fn,\n","    #model_dir=\"/tmp/mnist_convnet_model\"\n",")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using default config.\n","WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpyjey865o\n","INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpyjey865o', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f01e1a307b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"}]},{"metadata":{"id":"G8HOs5uolO8N","colab_type":"text"},"cell_type":"markdown","source":["### Set up logging for predictions.\n","\n","Log the values in the \"Softmax\" tensor with label \"probabilities\""]},{"metadata":{"id":"YV69DYwnlMXE","colab_type":"code","colab":{}},"cell_type":"code","source":["tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n","logging_hook = tf.train.LoggingTensorHook(\n","    tensors = tensors_to_log,\n","    every_n_iter = 50\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J8ZGkIhClaox","colab_type":"text"},"cell_type":"markdown","source":["###Train the model"]},{"metadata":{"id":"CidwCwopqeWh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"f75bc58c-b9c7-4e2c-c35e-73f4829aadca","executionInfo":{"status":"ok","timestamp":1549672254235,"user_tz":240,"elapsed":1290,"user":{"displayName":"Harvey Wang","photoUrl":"https://lh5.googleusercontent.com/-v6ZFtxMCliI/AAAAAAAAAAI/AAAAAAAABzQ/ghEL5GjlxHQ/s64/photo.jpg","userId":"14547178415595556415"}}},"cell_type":"code","source":[""],"execution_count":7,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"],"name":"stdout"}]},{"metadata":{"id":"1WTk96pYqZ4E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"948c7535-e26f-407d-9cbf-feb9d421dfd0","executionInfo":{"status":"ok","timestamp":1549672266677,"user_tz":240,"elapsed":1104,"user":{"displayName":"Harvey Wang","photoUrl":"https://lh5.googleusercontent.com/-v6ZFtxMCliI/AAAAAAAAAAI/AAAAAAAABzQ/ghEL5GjlxHQ/s64/photo.jpg","userId":"14547178415595556415"}}},"cell_type":"code","source":["input_layer = tf.reshape(train_data, [-1, 28, 28, 1])\n","print(input_layer)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Tensor(\"Reshape_1:0\", shape=(55000, 28, 28, 1), dtype=float32)\n"],"name":"stdout"}]},{"metadata":{"id":"Pnc1CR2AlbSe","colab_type":"code","outputId":"e1b7dbfd-2b62-47eb-a22c-4d2ebcf6972a","executionInfo":{"status":"ok","timestamp":1549381104131,"user_tz":240,"elapsed":215115,"user":{"displayName":"Harvey Wang","photoUrl":"https://lh5.googleusercontent.com/-v6ZFtxMCliI/AAAAAAAAAAI/AAAAAAAABzQ/ghEL5GjlxHQ/s64/photo.jpg","userId":"14547178415595556415"}},"colab":{"base_uri":"https://localhost:8080/","height":178350,"output_embedded_package_id":"1iBmjDNzUOqQM050V0pnlJRY6MQh9U7mC"}},"cell_type":"code","source":["train_input_fn = tf.estimator.inputs.numpy_input_fn(\n","    x = train_data,\n","    y = train_labels,\n","    batch_size = 100,\n","    num_epochs = None,\n","    shuffle = True\n",")\n","mnist_classifier.train(\n","    input_fn = train_input_fn,\n","    steps = 20000,\n","    hooks = [logging_hook]\n",")"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"metadata":{"id":"-mRbXr40mX6_","colab_type":"text"},"cell_type":"markdown","source":["###Evalute the model and print results"]},{"metadata":{"id":"hlWqYSqmme41","colab_type":"code","colab":{}},"cell_type":"code","source":["eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n","    x = {\"x\": eval_data},\n","    y = eval_labels,\n","    num_epochs = 1,\n","    shuffle = False\n",")\n","eval_results = mnist_classifier.evaluate(input_fn = eval_input_fn)\n","print(eval_results)"],"execution_count":0,"outputs":[]}]}