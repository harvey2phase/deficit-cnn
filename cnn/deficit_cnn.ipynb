{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deficit_cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "q3kqWRyfwc3Z"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvey2phase/summer-research/blob/master/cnn/deficit_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0DJNY2_onLW",
        "colab_type": "text"
      },
      "source": [
        "function enable_vim() {\n",
        "    document.querySelectorAll(\".CodeMirror\").forEach(\n",
        "        function (e) {\n",
        "            e.CodeMirror.setOption(\"vimMode\", true);\n",
        "            CodeMirror.Vim.map('jk', '<Esc>', 'insert')\n",
        "            CodeMirror.Vim.map('JK', '<Esc>', 'insert')\n",
        "            CodeMirror.Vim.map('jk', '<Esc>', 'visual')\n",
        "            CodeMirror.Vim.map('JK', '<Esc>', 'visual')\n",
        "        }\n",
        "    );\n",
        "}\n",
        "document.addEventListener(\n",
        "    'keydown', function(e) {\n",
        "        if (e.keyCode == 13 && e.metaKey || e.keyCode == 13 && e.shiftKey) {\n",
        "            for (var i = 0; i < 10; i++) setTimeout(enable_vim, 1000 * i);\n",
        "        }\n",
        "    }\n",
        ");\n",
        "enable_vim();\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BgtCd2okfpJ",
        "colab_type": "code",
        "outputId": "19e8d227-2e48-4ace-b3a6-dda26a99474f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "#import keras\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ria_Y5e7mlEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = \"/content/gdrive/My Drive/\"\n",
        "\n",
        "DATA_FOLDER = \"e4_prob_of_death_at_80/\"\n",
        "\n",
        "# Shape: [training_size, 50 * HISTORY]\n",
        "HISTORY = 20\n",
        "NODE_COUNT = 50\n",
        "\n",
        "TRAIN_SET = DATA_FOLDER + str(HISTORY) + \"x5y_train_set.txt\"\n",
        "TRAIN_LABELS = DATA_FOLDER + str(HISTORY) + \"x5y_train_labels.txt\"\n",
        "EVAL_SET = DATA_FOLDER + str(HISTORY) + \"x5y_eval_set.txt\"\n",
        "EVAL_LABELS = DATA_FOLDER + str(HISTORY) + \"x5y_eval_labels.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSBZxyZAqSOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STEPS = 1 * 10 ** 5\n",
        "LEARNING_RATE = 10 ** -4\n",
        "DROPOUT_RATE = 0.4\n",
        "BATCH_SIZE = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnAqZBPGkC2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONV1_FILTER = 32\n",
        "CONV1_SIZE = [5, 5]\n",
        "\n",
        "CONV2_FILTER = 64\n",
        "CONV2_SIZE = [5, 5]\n",
        "\n",
        "POOL_SIZE = [2, 2]\n",
        "POOL_STRIDE = 2\n",
        "\n",
        "DENSE_UNITS = 1024\n",
        "LOGITS_UNITS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3kqWRyfwc3Z",
        "colab_type": "text"
      },
      "source": [
        "###Silence Warnings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7RffwGmt3pO",
        "colab_type": "text"
      },
      "source": [
        "- 0: all messages are logged (default behavior)\n",
        "- 1: INFO messages are not printed\n",
        "- 2: INFO and WARNING messages are not printed\n",
        "- 3: INFO, WARNING, and ERROR messages are not printed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2N2_KNFwcfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXt8jbid4Zzd",
        "colab_type": "text"
      },
      "source": [
        "###Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNf32Dd6WpOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deficit_cnn_model(features, labels, mode):\n",
        "\n",
        "    ''' Input Layer '''\n",
        "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
        "    input_layer = tf.reshape(features, [-1, NODE_COUNT, HISTORY, 1])\n",
        "\n",
        "    ''' Convolutional Layer #1 '''\n",
        "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
        "    #   Padding is added to preserve width and height.\n",
        "    # Tensor Shape: [batch_size, 50, HISTORY, 32]\n",
        "    conv1 = tf.layers.conv2d(\n",
        "        inputs = input_layer,\n",
        "        filters = CONV1_FILTER,\n",
        "        kernel_size = CONV1_SIZE,\n",
        "        padding = \"same\",\n",
        "        activation = tf.nn.relu\n",
        "    )\n",
        "\n",
        "    ''' Pooling Layer #1 '''\n",
        "    # Max pooling layer with a 2x2 filter and stride of 2\n",
        "    # Tensor Shape: [batch_size, 25, HISTORY / 2, 32]\n",
        "    pool1 = tf.layers.max_pooling2d(\n",
        "        inputs = conv1,\n",
        "        pool_size = POOL_SIZE,\n",
        "        strides = POOL_STRIDE\n",
        "    )\n",
        "\n",
        "    ''' Convolutional Layer #2 '''\n",
        "    # Computes 64 features using a 5x5 filter.\n",
        "    #   Padding is added to preserve width and height.\n",
        "    # Tensor Shape: [batch_size, 25, HISTORY / 2, 64]\n",
        "    conv2 = tf.layers.conv2d(\n",
        "        inputs = pool1,\n",
        "        filters = CONV2_FILTER,\n",
        "        kernel_size = CONV2_SIZE,\n",
        "        padding = \"same\",\n",
        "        activation = tf.nn.relu\n",
        "    )\n",
        "\n",
        "    ''' Pooling Layer #2 '''\n",
        "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
        "    # Tensor Shape: [batch_size, 12, HISTORY / 4, 64]\n",
        "    pool2 = tf.layers.max_pooling2d(\n",
        "        inputs = conv2,\n",
        "        pool_size = POOL_SIZE,\n",
        "        strides = POOL_STRIDE\n",
        "    )\n",
        "\n",
        "    ''' Flatten tensor into a batch of vectors '''\n",
        "    # Output Tensor Shape: [batch_size, 12 * 64]\n",
        "    pool2_flat = tf.reshape(pool2, [-1, 12 * 64 * int(HISTORY / 4)])\n",
        "\n",
        "    ''' Dense Layer '''\n",
        "    # Densely connected layer with 1024 neurons\n",
        "    # Tensor Shape: [batch_size, 1024]\n",
        "    dense = tf.layers.dense(\n",
        "        inputs = pool2_flat,\n",
        "        units = DENSE_UNITS,\n",
        "        activation = tf.nn.relu\n",
        "    )\n",
        "    \n",
        "    ''' Add dropout operation '''\n",
        "    # 0.6 probability that element will be kept\n",
        "    # Tensor Shape: [batch_size, 1024]\n",
        "    dropout = tf.layers.dropout(\n",
        "        inputs = dense,\n",
        "        rate = DROPOUT_RATE,\n",
        "        training = mode == tf.estimator.ModeKeys.TRAIN\n",
        "    )\n",
        "\n",
        "\n",
        "    ''' Logits layer '''\n",
        "    # Input Tensor Shape: [batch_size, 1024]\n",
        "    # Tensor Shape: [batch_size, 1024]\n",
        "    logits = tf.layers.dense(\n",
        "        inputs = dropout,\n",
        "        units = LOGITS_UNITS\n",
        "    )\n",
        "\n",
        "    predictions = {\n",
        "        # Generate predictions (for PREDICT and EVAL mode)\n",
        "        \"classes\": tf.argmax(input = logits, axis = 1),\n",
        "\n",
        "        # Add `softmax_tensor` to the graph.\n",
        "        # It is used for PREDICT and by the `logging_hook`.\n",
        "        \"probabilities\": tf.nn.softmax(logits, name = \"softmax_tensor\")\n",
        "    }\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode = mode,\n",
        "            predictions = predictions\n",
        "        )\n",
        "\n",
        "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(\n",
        "        labels = labels,\n",
        "        logits = logits\n",
        "    )\n",
        "\n",
        "    # Configure the Training Op (for TRAIN mode)\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "        optimizer = tf.train.GradientDescentOptimizer(\n",
        "            learning_rate = LEARNING_RATE\n",
        "        )\n",
        "        train_op = optimizer.minimize(\n",
        "            loss = loss,\n",
        "            global_step = tf.train.get_global_step()\n",
        "        )\n",
        "\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "            mode = mode,\n",
        "            loss = loss,\n",
        "            train_op = train_op\n",
        "        )\n",
        "\n",
        "    # Add evaluation metrics (for EVAL mode)\n",
        "    eval_metric_ops = {\n",
        "        \"accuracy\": tf.metrics.accuracy(\n",
        "            labels = labels,\n",
        "            predictions = predictions[\"classes\"]\n",
        "        )\n",
        "    }\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode = mode,\n",
        "        loss = loss,\n",
        "        eval_metric_ops = eval_metric_ops\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BifdneGnqGz4",
        "colab_type": "text"
      },
      "source": [
        "##Load Data and Setup Logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peTWxyaU4e9N",
        "colab_type": "text"
      },
      "source": [
        "###Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHyUGD7uYm58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = np.loadtxt(PATH + TRAIN_SET)\n",
        "eval_data = np.loadtxt(PATH + EVAL_SET)\n",
        "\n",
        "train_labels = np.loadtxt(\n",
        "    PATH + TRAIN_LABELS,\n",
        "    dtype = np.int32\n",
        ")\n",
        "eval_labels = np.loadtxt(\n",
        "    PATH + EVAL_LABELS,\n",
        "    dtype = np.int32\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu8i3DsN4lHV",
        "colab_type": "text"
      },
      "source": [
        "###Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLYom4HUpW0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
        "logging_hook = tf.train.LoggingTensorHook(\n",
        "    tensors = tensors_to_log,\n",
        "    every_n_iter = 50\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AS-aCIavEc7",
        "colab_type": "text"
      },
      "source": [
        "##Create Estimator and Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9d2TMXRpqDe",
        "colab_type": "text"
      },
      "source": [
        "###Create Estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAKzY479Y0P0",
        "colab_type": "code",
        "outputId": "44adbbd1-f07c-4460-f882-bc78f86d0152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "classifier = tf.estimator.Estimator(model_fn = deficit_cnn_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 16:04:52.512900 140001776846720 estimator.py:1811] Using temporary folder as model directory: /tmp/tmpjrmqhofs\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "miR0udQm7dNe"
      },
      "source": [
        "###Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tb-Ua4Jh7e1t",
        "colab": {}
      },
      "source": [
        "# Shape: [606, 50, HISTORY, 1]\n",
        "input_layer = tf.reshape(train_data, [-1, NODE_COUNT, HISTORY, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG8tnOD-7brM",
        "colab_type": "code",
        "outputId": "4935554e-b277-4b44-a65c-0dd70cbf00a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(train_data.shape)\n",
        "print(train_labels.shape)\n",
        "print(eval_data.shape)\n",
        "print(eval_labels.shape)\n",
        "print(input_layer.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2126, 1000)\n",
            "(2126,)\n",
            "(1000, 1000)\n",
            "(1000,)\n",
            "(2126, 50, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-g_WPM1M7R3G",
        "outputId": "435d1bb3-9371-4def-b3bf-fbe0dd24afa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x = train_data,\n",
        "    y = train_labels,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    num_epochs = None,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "classifier.train(\n",
        "    input_fn = train_input_fn,\n",
        "    steps = STEPS,\n",
        "    #hooks = [logging_hook]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0627 19:42:51.204631 139635830896512 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0627 19:42:51.220952 139635830896512 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0627 19:42:51.223834 139635830896512 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0627 19:42:51.235998 139635830896512 deprecation.py:323] From <ipython-input-5-41469d7a9ea7>:16: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "W0627 19:42:51.239010 139635830896512 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0627 19:42:51.449787 139635830896512 deprecation.py:323] From <ipython-input-5-41469d7a9ea7>:25: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "W0627 19:42:51.591318 139635830896512 deprecation.py:323] From <ipython-input-5-41469d7a9ea7>:59: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0627 19:42:51.864712 139635830896512 deprecation.py:323] From <ipython-input-5-41469d7a9ea7>:68: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0627 19:42:51.946798 139635830896512 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 19:42:53.026972 139635830896512 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss6A-67c19CE",
        "colab_type": "text"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz7EBVAJY659",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the model and print results\n",
        "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x = eval_data,\n",
        "    y = eval_labels,\n",
        "    num_epochs = 1,\n",
        "    shuffle = False\n",
        ")\n",
        "eval_results = classifier.evaluate(input_fn = eval_input_fn)\n",
        "print(eval_results)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}