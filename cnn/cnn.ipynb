{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7fcY2b7xlsIQ"
   },
   "source": [
    "## Mount Drives, Import Packages, Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36464,
     "status": "ok",
     "timestamp": 1557510816458,
     "user": {
      "displayName": "Harvey Wang",
      "photoUrl": "https://lh5.googleusercontent.com/-v6ZFtxMCliI/AAAAAAAAAAI/AAAAAAAABzQ/ghEL5GjlxHQ/s64/photo.jpg",
      "userId": "14547178415595556415"
     },
     "user_tz": 180
    },
    "id": "GmYpfOzcpK2j",
    "outputId": "273a460e-d87e-4a10-b9ef-c550c5b1b3c7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "research_path = '/content/gdrive/Computers/My MacBook Pro/Summer Research/'\n",
    "data_path = research_path + 'processed_data/matrix_50nodes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-BM3_VHIjup"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tr7I0fuY46ds"
   },
   "outputs": [],
   "source": [
    "#tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbHvrLUNg4U0"
   },
   "source": [
    "##Model function for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXvm1LOb9NI6"
   },
   "source": [
    "###Mnist CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BlpWhnZEj1u"
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel\n",
    "    input_layer = tf.reshape(features, [-1, 28, 28, 1])\n",
    "    #input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "    \n",
    "    print(input_layer)\n",
    "    \n",
    "    ''' Convolutional Layer #1'''\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs = input_layer,\n",
    "        filters = 32,\n",
    "        kernel_size = [5, 5],\n",
    "        padding = \"same\",\n",
    "        activation = tf.nn.relu\n",
    "    )\n",
    "\n",
    "    ''' Pooling Layer #1 '''\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        inputs = conv1,\n",
    "        pool_size = [2, 2],\n",
    "        strides = 2\n",
    "    )\n",
    "    \n",
    "    ''' Convolutional Layer #2 '''\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs = pool1,\n",
    "        filters = 64,\n",
    "        kernel_size = [5, 5],\n",
    "        padding = \"same\",\n",
    "        activation = tf.nn.relu\n",
    "    )\n",
    "\n",
    "    ''' Pooling Layer #2 '''\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "        inputs = conv2,\n",
    "        pool_size = [2, 2],\n",
    "        strides = 2\n",
    "    )\n",
    "\n",
    "    ''' Flatten tensor into a batch of vectors '''\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    ''' Dense Layer '''\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(\n",
    "        inputs = pool2_flat,\n",
    "        units = 1024,\n",
    "        activation = tf.nn.relu\n",
    "    )\n",
    "\n",
    "    ''' Add dropout operation '''\n",
    "    # 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs = dense,\n",
    "        rate = 0.4,\n",
    "        training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    )\n",
    "\n",
    "\n",
    "    ''' Logits layer '''\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(\n",
    "        inputs = dropout,\n",
    "        units = 10\n",
    "    )\n",
    "\n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \n",
    "        # Add `softmax_tensor` to the graph.\n",
    "        # It is used for PREDICT and by the `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss = loss,\n",
    "            global_step = tf.train.get_global_step()\n",
    "        )\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode = mode,\n",
    "            loss = loss,\n",
    "            train_op = train_op\n",
    "        )\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels = labels,\n",
    "            predictions = predictions[\"classes\"]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        loss = loss,\n",
    "        eval_metric_ops = eval_metric_ops\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H_iLx32nUkaX"
   },
   "source": [
    "### Deficits CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PDfkCpG5g5Cm"
   },
   "outputs": [],
   "source": [
    "def deficit_cnn_model(features, labels, mode):\n",
    "\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel\n",
    "    input_layer = tf.reshape(features, [-1, 50, 120, 1])\n",
    "\n",
    "    ''' Convolutional Layer #1'''\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 50, 120, 1]\n",
    "    # Output Tensor Shape: [batch_size, 50, 120, 32]\n",
    "    conv1 = keras.layers.conv2d(\n",
    "        inputs = input_layer,\n",
    "        filters = 32,\n",
    "        kernel_size = [5, 5],\n",
    "        padding = \"same\",\n",
    "        activation = tf.nn.relu\n",
    "    )\n",
    "\n",
    "    ''' Pooling Layer #1 '''\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        inputs = conv1,\n",
    "        pool_size = [2, 2],\n",
    "        strides = 2\n",
    "    )\n",
    "    \n",
    "    ''' Convolutional Layer #2 '''\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs = pool1,\n",
    "        filters = 64,\n",
    "        kernel_size = [5, 5],\n",
    "        padding = \"same\",\n",
    "        activation = tf.nn.relu\n",
    "    )\n",
    "\n",
    "    ''' Pooling Layer #2 '''\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "        inputs = conv2,\n",
    "        pool_size = [2, 2],\n",
    "        strides = 2\n",
    "    )\n",
    "\n",
    "    ''' Flatten tensor into a batch of vectors '''\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    ''' Dense Layer '''\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(\n",
    "        inputs = pool2_flat,\n",
    "        units = 1024,\n",
    "        activation = tf.nn.relu\n",
    "    )\n",
    "\n",
    "    ''' Add dropout operation '''\n",
    "    # 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs = dense,\n",
    "        rate = 0.4,\n",
    "        training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    )\n",
    "    \n",
    "\n",
    "    ''' Logits layer '''\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(\n",
    "        inputs = dropout,\n",
    "        units = 10\n",
    "    )\n",
    "\n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \n",
    "        # Add `softmax_tensor` to the graph.\n",
    "        # It is used for PREDICT and by the `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss = loss,\n",
    "            global_step = tf.train.get_global_step()\n",
    "        )\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode = mode,\n",
    "            loss = loss,\n",
    "            train_op = train_op\n",
    "        )\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels = labels,\n",
    "            predictions = predictions[\"classes\"]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        loss = loss,\n",
    "        eval_metric_ops = eval_metric_ops\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSyPV9W0k6JZ"
   },
   "source": [
    "## Load Data\n",
    "Load training and evaluation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ha6B6AxD-FjX"
   },
   "source": [
    "### Mnist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 704,
     "status": "ok",
     "timestamp": 1557507240397,
     "user": {
      "displayName": "Harvey Wang",
      "photoUrl": "https://lh5.googleusercontent.com/-v6ZFtxMCliI/AAAAAAAAAAI/AAAAAAAABzQ/ghEL5GjlxHQ/s64/photo.jpg",
      "userId": "14547178415595556415"
     },
     "user_tz": 180
    },
    "id": "u4HnQSblVGPP",
    "outputId": "c824074f-2c2a-4aaa-e740-97c97481ae4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-6498ac1de7b8>:1: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data.\n",
      "WARNING:tensorflow:From /anaconda3/envs/Tensorflow/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda3/envs/Tensorflow/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda3/envs/Tensorflow/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /anaconda3/envs/Tensorflow/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/envs/Tensorflow/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/envs/Tensorflow/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTvGEY1W-MnJ"
   },
   "source": [
    "### Deficit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grRq6ptz_j5I"
   },
   "source": [
    "####Load individual data at death age $80 < d < 90$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8Ed_A9yl-NC"
   },
   "outputs": [],
   "source": [
    "# filename = '69.txt'\n",
    "data = \n",
    "\n",
    "# ind = np.loadtxt(path+filename)\n",
    "\n",
    "# time = ind[:, 0]\n",
    "# node = ind[:, 1]\n",
    "# state = ind[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zI-hagvS-2YD"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNyeOj71lHIC"
   },
   "source": [
    "## Create the Estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEfLJfhbkvTS"
   },
   "outputs": [],
   "source": [
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn = cnn_model_fn,\n",
    "    #model_dir=\"/tmp/mnist_convnet_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7WvBkAj_BEi"
   },
   "source": [
    "##Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8HOs5uolO8N"
   },
   "source": [
    "### Set up logging for predictions.\n",
    "\n",
    "Log the values in the \"Softmax\" tensor with label \"probabilities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YV69DYwnlMXE"
   },
   "outputs": [],
   "source": [
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors = tensors_to_log,\n",
    "    every_n_iter = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J8ZGkIhClaox"
   },
   "source": [
    "###Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1WTk96pYqZ4E"
   },
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(train_data, [-1, 28, 28, 1])\n",
    "print(input_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pnc1CR2AlbSe"
   },
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x = train_data,\n",
    "    y = train_labels,\n",
    "    batch_size = 100,\n",
    "    num_epochs = None,\n",
    "    shuffle = True\n",
    ")\n",
    "mnist_classifier.train(\n",
    "    input_fn = train_input_fn,\n",
    "    steps = 20000,\n",
    "    hooks = [logging_hook]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-mRbXr40mX6_"
   },
   "source": [
    "###Evalute the model and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hlWqYSqmme41"
   },
   "outputs": [],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x = {\"x\": eval_data},\n",
    "    y = eval_labels,\n",
    "    num_epochs = 1,\n",
    "    shuffle = False\n",
    ")\n",
    "eval_results = mnist_classifier.evaluate(input_fn = eval_input_fn)\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "fbHvrLUNg4U0",
    "FXvm1LOb9NI6",
    "RNyeOj71lHIC",
    "w7WvBkAj_BEi"
   ],
   "name": "cnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
